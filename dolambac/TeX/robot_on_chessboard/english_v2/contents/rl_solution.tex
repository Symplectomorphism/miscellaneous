\subsection{RL solution}
\label{ssec:rl_sol}
%
We want to be able to programmatically solve this toy problem so that we can 
find the solution for any given initial state. We can use reinforcement learning
(RL) to solve this problem. 

The reasoning goes as follows: we want to apply RL techniques, such as double
$Q$-learning~\cite{morales2020grokking} to find the action-value function and
from it the state-value function. However, in order to do this, we need to
define the state, action, transition, and reward functions. The key to solving 
this problem efficiently with RL methods is to observe that the state does not 
only comprise the position of the robot on the chessboard, but also the ``world 
belief''. Here, we define world belief as our current belief of mapping of the 
arrow keys to the actual directions.

Let $\uline{N} = \{0, 1, \ldots, N-1\}$ denote the set of nonnegative integers
ranging from $0$ to $N-1$. We define the Markov decision process (MDP) as
follows:

\begin{itemize}
\item \textbf{State space}: The product $\mc{S} = \uline{4} \times \uline{4}
\times \uline{24}$. The first two factors product stands for the position of the
robot in the grid world and the third factor to the ID number of the world, of
which there are $4! = 24$. These $24$ worlds correspond to the possible ways to
permute the four key directions.
\item \textbf{Action space}: The product $\mc{A} = \uline{4} \times \uline{24}$.
The first factor corresponds to a particular direction to move the robot. The
second factor corresponds to which of the $24$ worlds to move the current belief
to. Of course, any direction chosen in this set would be the actual direction
the robot would move only if there is overlap between the assumed and correct 
directions.
\item \textbf{Transition function}: This is a deterministic function of the true
world. If the robot is at position $(i, j)$ and the world belief is $w$, then1
\item \textbf{Reward function}: A negative reward (penalty) of $-10$ is
incurrent whenever $s \in \mc{S}$ is nonterminal. Otherwise, this reward is $0$.
Furthermore, for each overlap between the assumed and correct directions
the agent is rewarded $+1$.
\end{itemize}

Our implementation~\cite{saticiGitHub} of this MDP is carried out using 
Gymnasium~\cite{towers2024gymnasium}, a Python library for RL.