{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import gymnasium as gym\n",
    "from gridworld.envs import SiblingGridWorldEnv\n",
    "from gymnasium.envs.registration import register\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from tabulate import tabulate\n",
    "import tqdm as tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "SEEDS = (12, 34, 56, 78, 90)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import *\n",
    "from sibling_gw_agent import SiblingGWAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "params = {\n",
    "    'figure.figsize': (15, 8),\n",
    "    'font.size': 24,\n",
    "    'legend.fontsize': 20,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 24,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "layout = {\n",
    "    \"Training\": {\n",
    "        \"Q_gw\": [\"Multiline\", [\"Q_gw/center\", \"Q_gw/corner\"]],\n",
    "        \"Q_bandit\": [\"Multiline\", [\"Q_bandit/zero\", \"Q_bandit/twenty\"]],\n",
    "    },\n",
    "}\n",
    "\n",
    "writer = SummaryWriter('runs/sibling_gw')\n",
    "writer.add_custom_scalars(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "def sibling():\n",
    "    env = SiblingGridWorldEnv(P_gridworld)\n",
    "    env = TimeLimit(env, max_episode_steps=100)\n",
    "#     env = RelativePositionenv)\n",
    "    return env\n",
    "\n",
    "register(\n",
    "    id='SiblingGridWorld-v0',\n",
    "    entry_point=sibling,\n",
    "    max_episode_steps=100,\n",
    ")\n",
    "\n",
    "env = gym.make('SiblingGridWorld-v0')\n",
    "env = env.unwrapped\n",
    "# env.render_mode = 'human'\n",
    "obs, info = env.reset(options={'randomize_world': True})\n",
    "print(env._true_world_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100_000\n",
    "agent = SiblingGWAgent(env, gamma=1.0, \n",
    "            init_alpha=0.5, min_alpha=0.05, alpha_decay_ratio=0.5, \n",
    "            init_epsilon=1.0, min_epsilon=0.1, epsilon_decay_ratio=0.9, \n",
    "            n_episodes=n_episodes)\n",
    "# agent = SiblingGWAgent(env, min_epsilon=0.5, epsilon_decay_ratio=0.9, n_episodes=n_episodes)\n",
    "# agent.episode = 5000\n",
    "# agent.epsilons[agent.episode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [02:56<00:00, 566.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(agent.episode, agent.episode + n_episodes)):\n",
    "    agent.episode = episode\n",
    "    state, info = env.reset()\n",
    "    # if episode % 3_500 == 0:\n",
    "    #     state, info = env.reset(options={'randomize_world': True})\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # update the agent\n",
    "        agent.update(state, action, reward, terminated, next_state)\n",
    "\n",
    "        # update if the environment is done or truncated\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        writer.add_scalar(\"Q_gw/center\",\n",
    "            np.max(agent.Q_gw[agent.state_multi_to_lin(np.array([2, 2]))]), \n",
    "            episode\n",
    "        )\n",
    "        writer.add_scalar(\"Q_gw/corner\",\n",
    "            np.max(agent.Q_gw[agent.state_multi_to_lin(np.array([0, 0]))]), \n",
    "            episode\n",
    "        )\n",
    "        writer.add_scalar(\"Q_bandit/zero\",\n",
    "            np.max(agent.Q_bandit[0]), \n",
    "            episode\n",
    "        )\n",
    "        writer.add_scalar(\"Q_bandit/twenty\",\n",
    "            np.max(agent.Q_bandit[20]), \n",
    "            episode\n",
    "        )\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/999 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SiblingGWAgent' object has no attribute 'greedy_action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1_000\u001b[39m)):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_ep_len):\n\u001b[0;32m---> 14\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_action\u001b[49m(obs)\n\u001b[1;32m     15\u001b[0m         obs, rew, done, trunc, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# print(obs, rew, info)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# time.sleep(0.05)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SiblingGWAgent' object has no attribute 'greedy_action'"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng()\n",
    "env.render_mode = None\n",
    "# obs, info = env.reset(options={'randomize_world': True})\n",
    "obs, _ = env.reset()\n",
    "env._agent_location = np.array([2, 2])\n",
    "obs = env._agent_location\n",
    "\n",
    "avg_steps = 100\n",
    "alpha = 0.1\n",
    "running_avg = [10]\n",
    "max_ep_len = 100\n",
    "for e in tqdm(range(1, 1_000)):\n",
    "    for i in range(1, max_ep_len):\n",
    "        action = agent.greedy_action(obs)\n",
    "        obs, rew, done, trunc, info = env.step(action)\n",
    "        # print(obs, rew, info)\n",
    "        # time.sleep(0.05)\n",
    "        if done or i == max_ep_len - 1:\n",
    "            # print(f\"Finished in {i} steps.\")\n",
    "            avg_steps += 1/e * (i - avg_steps)\n",
    "            running_avg += [(1-alpha) * running_avg[-1] + alpha * avg_steps]\n",
    "            break\n",
    "    obs, _ = env.reset()\n",
    "    env._agent_location = np.array([2, 2])\n",
    "    obs = env._agent_location\n",
    "    # if e % 100 == 0:\n",
    "    #     print(f\"Episode {e} finished, current average estimate is {avg_steps}.\")\n",
    "\n",
    "print(f\"Cumulative average estimate: {avg_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([2, 2, 11])\n",
    "state_idx = np.ravel_multi_index(state, env.observation_space.nvec, order='F')\n",
    "print(state_idx, agent.Q[state_idx].max(), np.argmax(agent.Q[state_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render_mode = 'human'\n",
    "obs, info = env.reset()\n",
    "env._agent_location = np.array([2, 2])\n",
    "obs[:-1] = env._agent_location\n",
    "# print(np.concatenate([env._agent_location, env._world_belief]))\n",
    "env._render_frame()\n",
    "# time.sleep(1.0)\n",
    "print(env._true_world_idx)\n",
    "\n",
    "for i in range(1,max_ep_len+1):\n",
    "    obs_idx = np.ravel_multi_index(obs, env.observation_space.nvec, order='F')\n",
    "    action_linear = np.argmax(agent.Q[obs_idx])\n",
    "    action = np.unravel_index(action_linear, env.action_space.nvec, order='F')\n",
    "    # print(obs_idx, np.concatenate([env._agent_location, env._world_belief]), action_linear, action, agent.Q[obs_idx].max())\n",
    "    obs, rew, done, trunc, info = env.step(action)\n",
    "    # print(obs, rew, info)\n",
    "    # time.sleep(0.05)\n",
    "    if done or trunc:\n",
    "        print(f\"Finished in {i} steps.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.act_lin_to_multi(np.argmax(agent.Q[agent.state_multi_to_lin(np.array([2, 2, 0]))])))\n",
    "print(agent.act_lin_to_multi(np.argmax(agent.Q[agent.state_multi_to_lin(np.array([2, 2, 7]))])))\n",
    "print(agent.act_lin_to_multi(np.argmax(agent.Q[agent.state_multi_to_lin(np.array([2, 2, 11]))])))\n",
    "print(agent.act_lin_to_multi(np.argmax(agent.Q[agent.state_multi_to_lin(np.array([2, 2, 20]))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env._true_world_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Q[agent.state_multi_to_lin(np.array([2, 2, 0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((4*24,), dtype=np.float32)*-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
